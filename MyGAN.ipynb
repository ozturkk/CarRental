{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ozturkk/CarRental/blob/main/MyGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ymp4K4e1Sw6I"
      },
      "source": [
        "İmports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BZAgYwGUFVt",
        "outputId": "218072e7-bd6a-49f6-8edd-9130ef833506"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8qVPVHhUnj7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/gdrive/My Drive/GanCalisma/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Digt1EWFUrpR",
        "outputId": "644d1cab-a70a-4d7a-80ea-64d65f92967e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "combined_model.png  encoder.png  facades_data1\t   generator.png  SavedModels\n",
            "discriminator.png   facades\t facades_data.zip  MyGAN.ipynb\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0euU0jATEWd",
        "outputId": "b4b89f85-d656-4075-fd94-ad71f4cd1c35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 33.5 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 41.1 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 49.4 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 37.8 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 33.4 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61 kB 37.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 31.4 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 32.7 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 92 kB 35.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 122 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 153 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 184 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 204 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 215 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 235 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 245 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 266 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 296 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 307 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 317 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 327 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 337 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 358 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 368 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 378 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 389 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 399 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 409 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 419 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 430 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 440 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 460 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 471 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 481 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 491 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 501 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 512 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 522 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 532 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 542 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 552 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 563 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 573 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 583 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 593 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 604 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 614 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 634 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 645 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 655 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 665 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 675 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 686 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 696 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 706 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 716 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 727 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 737 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 747 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 757 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 768 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 778 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 788 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 798 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 808 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 819 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 829 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 839 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 849 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 860 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 870 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 880 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 890 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 901 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 911 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 921 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 931 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 942 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 952 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 962 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 972 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 983 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 993 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0 MB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0 MB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0 MB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0 MB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.0 MB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1 MB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1 MB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1 MB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.1 MB 32.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1 MB 32.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 32.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qqq tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIXPF61xV0K2",
        "outputId": "c7589ab9-67b7-45f3-f0d5-a00ef885ae64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.8/dist-packages (0.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install glob2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5e8hSWNSz0z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.utils import plot_model\n",
        "\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzYJhn2qTThm",
        "outputId": "cc49e1ce-6394-4f38-8bd4-26807b7994ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total samples: 1000.\n",
            "Total training samples: 700.\n",
            "Total validation samples: 300.\n",
            "Segmentation map batch shape: (2, 512, 512, 3).\n",
            "Image batch shape: (2, 512, 512, 3).\n",
            "One-hot encoded label map shape: (2, 512, 512, 19).\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_28/kernel:0', 'conv2d_29/kernel:0', 'instance_normalization_17/gamma:0', 'instance_normalization_17/beta:0', 'conv2d_30/kernel:0', 'instance_normalization_18/gamma:0', 'instance_normalization_18/beta:0', 'conv2d_31/kernel:0', 'instance_normalization_19/gamma:0', 'instance_normalization_19/beta:0', 'conv2d_32/kernel:0', 'instance_normalization_20/gamma:0', 'instance_normalization_20/beta:0', 'mean/kernel:0', 'mean/bias:0', 'variance/kernel:0', 'variance/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_28/kernel:0', 'conv2d_29/kernel:0', 'instance_normalization_17/gamma:0', 'instance_normalization_17/beta:0', 'conv2d_30/kernel:0', 'instance_normalization_18/gamma:0', 'instance_normalization_18/beta:0', 'conv2d_31/kernel:0', 'instance_normalization_19/gamma:0', 'instance_normalization_19/beta:0', 'conv2d_32/kernel:0', 'instance_normalization_20/gamma:0', 'instance_normalization_20/beta:0', 'mean/kernel:0', 'mean/bias:0', 'variance/kernel:0', 'variance/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "350/350 [==============================] - 762s 2s/step - disc_loss: 0.9790 - gen_loss: 106.8666 - feat_loss: 7.9250 - vgg_loss: 9.8322 - kl_loss: 88.8800 - val_disc_loss: 0.9929 - val_gen_loss: 107.4337 - val_feat_loss: 8.6292 - val_vgg_loss: 9.8404 - val_kl_loss: 88.7216\n",
            "Epoch 2/200\n",
            "350/350 [==============================] - 743s 2s/step - disc_loss: 0.6937 - gen_loss: 108.5304 - feat_loss: 9.5409 - vgg_loss: 9.6628 - kl_loss: 88.5287 - val_disc_loss: 0.8632 - val_gen_loss: 107.4004 - val_feat_loss: 9.5897 - val_vgg_loss: 9.7479 - val_kl_loss: 88.5919\n",
            "Epoch 3/200\n",
            "350/350 [==============================] - 759s 2s/step - disc_loss: 0.5703 - gen_loss: 109.3915 - feat_loss: 10.1660 - vgg_loss: 9.6703 - kl_loss: 88.4923 - val_disc_loss: 0.5518 - val_gen_loss: 109.3991 - val_feat_loss: 10.0350 - val_vgg_loss: 9.5476 - val_kl_loss: 88.8700\n",
            "Epoch 4/200\n",
            "350/350 [==============================] - 746s 2s/step - disc_loss: 0.5274 - gen_loss: 109.8762 - feat_loss: 10.1673 - vgg_loss: 9.7606 - kl_loss: 88.8748 - val_disc_loss: 0.4920 - val_gen_loss: 109.2995 - val_feat_loss: 9.5895 - val_vgg_loss: 9.5756 - val_kl_loss: 88.5879\n",
            "Epoch 5/200\n",
            "350/350 [==============================] - 745s 2s/step - disc_loss: 0.5439 - gen_loss: 109.2118 - feat_loss: 9.8728 - vgg_loss: 9.5773 - kl_loss: 88.6781 - val_disc_loss: 0.5550 - val_gen_loss: 107.4452 - val_feat_loss: 8.9910 - val_vgg_loss: 9.1135 - val_kl_loss: 88.3329\n",
            "Epoch 6/200\n",
            "350/350 [==============================] - 761s 2s/step - disc_loss: 0.5192 - gen_loss: 108.3880 - feat_loss: 9.3024 - vgg_loss: 9.3243 - kl_loss: 88.7147 - val_disc_loss: 0.4790 - val_gen_loss: 108.6644 - val_feat_loss: 8.6759 - val_vgg_loss: 9.3518 - val_kl_loss: 89.3529\n",
            "Epoch 7/200\n",
            "350/350 [==============================] - 746s 2s/step - disc_loss: 0.5563 - gen_loss: 108.4669 - feat_loss: 9.3023 - vgg_loss: 9.3328 - kl_loss: 88.8357 - val_disc_loss: 0.5784 - val_gen_loss: 107.8945 - val_feat_loss: 9.1101 - val_vgg_loss: 9.2416 - val_kl_loss: 88.8733\n",
            "Epoch 8/200\n",
            "350/350 [==============================] - 747s 2s/step - disc_loss: 0.6088 - gen_loss: 108.5157 - feat_loss: 9.3450 - vgg_loss: 9.2475 - kl_loss: 88.9443 - val_disc_loss: 0.5708 - val_gen_loss: 107.1186 - val_feat_loss: 8.5434 - val_vgg_loss: 9.0965 - val_kl_loss: 88.8455\n",
            "Epoch 9/200\n",
            "184/350 [==============>...............] - ETA: 4:53 - disc_loss: 0.5766 - gen_loss: 108.2795 - feat_loss: 9.1649 - vgg_loss: 9.4318 - kl_loss: 88.6894"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-92b8d8a62490>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0mgaugan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGauGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_HEIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[0mgaugan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m history = gaugan.fit(\n\u001b[0m\u001b[1;32m    557\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "PATH = \"./facades/\"\n",
        "SPLIT = 0.3\n",
        "\n",
        "files = glob(PATH + \"*.jpg\")\n",
        "np.random.shuffle(files)\n",
        "\n",
        "split_index = int(len(files) * (1 - SPLIT))\n",
        "train_files = files[:split_index]\n",
        "val_files = files[split_index:]\n",
        "\n",
        "print(f\"Total samples: {len(files)}.\")\n",
        "print(f\"Total training samples: {len(train_files)}.\")\n",
        "print(f\"Total validation samples: {len(val_files)}.\")\n",
        "\n",
        "BATCH_SIZE = 2\n",
        "IMG_HEIGHT = IMG_WIDTH = 512\n",
        "NUM_CLASSES = 19\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "\n",
        "def load(image_files, batch_size, is_train=True):\n",
        "    def _random_crop(\n",
        "            segmentation_map, image, labels, crop_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    ):\n",
        "        crop_size = tf.convert_to_tensor(crop_size)\n",
        "        image_shape = tf.shape(image)[:2]\n",
        "        margins = image_shape - crop_size\n",
        "        y1 = tf.random.uniform(shape=(), maxval=margins[0], dtype=tf.int32)\n",
        "        x1 = tf.random.uniform(shape=(), maxval=margins[1], dtype=tf.int32)\n",
        "        y2 = y1 + crop_size[0]\n",
        "        x2 = x1 + crop_size[1]\n",
        "\n",
        "        cropped_images = []\n",
        "        images = [segmentation_map, image, labels]\n",
        "        for img in images:\n",
        "            cropped_images.append(img[y1:y2, x1:x2])\n",
        "        return cropped_images\n",
        "\n",
        "    def _load_data_tf(image_file, segmentation_map_file, label_file):\n",
        "        image = tf.image.decode_png(tf.io.read_file(image_file), channels=3)\n",
        "        segmentation_map = tf.image.decode_png(\n",
        "            tf.io.read_file(segmentation_map_file), channels=3\n",
        "        )\n",
        "        labels = tf.image.decode_bmp(tf.io.read_file(label_file), channels=0)\n",
        "        labels = tf.squeeze(labels)\n",
        "\n",
        "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
        "        segmentation_map = tf.cast(segmentation_map, tf.float32) / 127.5 - 1\n",
        "        return segmentation_map, image, labels\n",
        "\n",
        "    segmentation_map_files = [\n",
        "        image_file.replace(\"images\", \"segmentation_map\").replace(\"jpg\", \"png\")\n",
        "        for image_file in image_files\n",
        "    ]\n",
        "    label_files = [\n",
        "        image_file.replace(\"images\", \"segmentation_labels\").replace(\"jpg\", \"bmp\")\n",
        "        for image_file in image_files\n",
        "    ]\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (image_files, segmentation_map_files, label_files)\n",
        "    )\n",
        "\n",
        "    dataset = dataset.shuffle(batch_size * 10) if is_train else dataset\n",
        "    dataset = dataset.map(_load_data_tf, num_parallel_calls=AUTOTUNE)\n",
        "    dataset = dataset.map(_random_crop, num_parallel_calls=AUTOTUNE)\n",
        "    dataset = dataset.map(\n",
        "        lambda x, y, z: (x, y, tf.one_hot(z, NUM_CLASSES)), num_parallel_calls=AUTOTUNE\n",
        "    )\n",
        "    return dataset.batch(batch_size, drop_remainder=True)\n",
        "\n",
        "\n",
        "train_dataset = load(train_files, batch_size=BATCH_SIZE, is_train=True)\n",
        "val_dataset = load(val_files, batch_size=BATCH_SIZE, is_train=False)\n",
        "\n",
        "sample_train_batch = next(iter(train_dataset))\n",
        "print(f\"Segmentation map batch shape: {sample_train_batch[0].shape}.\")\n",
        "print(f\"Image batch shape: {sample_train_batch[1].shape}.\")\n",
        "print(f\"One-hot encoded label map shape: {sample_train_batch[2].shape}.\")\n",
        "\n",
        "\n",
        "# Plot a view samples from the training set.\n",
        "# for segmentation_map, real_image in zip(sample_train_batch[0], sample_train_batch[1]):\n",
        "#     fig = plt.figure(figsize=(10, 10))\n",
        "#     fig.add_subplot(1, 2, 1).set_title(\"Segmentation Map\")\n",
        "#     plt.imshow((segmentation_map + 1) / 2)\n",
        "#     fig.add_subplot(1, 2, 2).set_title(\"Real Image\")\n",
        "#     plt.imshow((real_image + 1) / 2)\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "class SPADE(layers.Layer):\n",
        "    def __init__(self, filters, epsilon=1e-5, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.epsilon = epsilon\n",
        "        self.conv = layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\")\n",
        "        self.conv_gamma = layers.Conv2D(filters, 3, padding=\"same\")\n",
        "        self.conv_beta = layers.Conv2D(filters, 3, padding=\"same\")\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.resize_shape = input_shape[1:3]\n",
        "\n",
        "    def call(self, input_tensor, raw_mask):\n",
        "        mask = tf.image.resize(raw_mask, self.resize_shape, method=\"nearest\")\n",
        "        x = self.conv(mask)\n",
        "        gamma = self.conv_gamma(x)\n",
        "        beta = self.conv_beta(x)\n",
        "        mean, var = tf.nn.moments(input_tensor, axes=(0, 1, 2), keepdims=True)\n",
        "        std = tf.sqrt(var + self.epsilon)\n",
        "        normalized = (input_tensor - mean) / std\n",
        "        output = gamma * normalized + beta\n",
        "        return output\n",
        "\n",
        "\n",
        "class ResBlock(layers.Layer):\n",
        "    def __init__(self, filters, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_filter = input_shape[-1]\n",
        "        self.spade_1 = SPADE(input_filter)\n",
        "        self.spade_2 = SPADE(self.filters)\n",
        "        self.conv_1 = layers.Conv2D(self.filters, 3, padding=\"same\")\n",
        "        self.conv_2 = layers.Conv2D(self.filters, 3, padding=\"same\")\n",
        "        self.learned_skip = False\n",
        "\n",
        "        if self.filters != input_filter:\n",
        "            self.learned_skip = True\n",
        "            self.spade_3 = SPADE(input_filter)\n",
        "            self.conv_3 = layers.Conv2D(self.filters, 3, padding=\"same\")\n",
        "\n",
        "    def call(self, input_tensor, mask):\n",
        "        x = self.spade_1(input_tensor, mask)\n",
        "        x = self.conv_1(tf.nn.leaky_relu(x, 0.2))\n",
        "        x = self.spade_2(x, mask)\n",
        "        x = self.conv_2(tf.nn.leaky_relu(x, 0.2))\n",
        "        skip = (\n",
        "            self.conv_3(tf.nn.leaky_relu(self.spade_3(input_tensor, mask), 0.2))\n",
        "            if self.learned_skip\n",
        "            else input_tensor\n",
        "        )\n",
        "        output = skip + x\n",
        "        return output\n",
        "\n",
        "\n",
        "class GaussianSampler(layers.Layer):\n",
        "    def __init__(self, batch_size, latent_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.batch_size = batch_size\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        means, variance = inputs\n",
        "        epsilon = tf.random.normal(\n",
        "            shape=(self.batch_size, self.latent_dim), mean=0.0, stddev=1.0\n",
        "        )\n",
        "        samples = means + tf.exp(0.5 * variance) * epsilon\n",
        "        return samples\n",
        "\n",
        "\n",
        "def downsample(\n",
        "        channels,\n",
        "        kernels,\n",
        "        strides=2,\n",
        "        apply_norm=True,\n",
        "        apply_activation=True,\n",
        "        apply_dropout=False,\n",
        "):\n",
        "    block = keras.Sequential()\n",
        "    block.add(\n",
        "        layers.Conv2D(\n",
        "            channels,\n",
        "            kernels,\n",
        "            strides=strides,\n",
        "            padding=\"same\",\n",
        "            use_bias=False,\n",
        "            kernel_initializer=keras.initializers.GlorotNormal(),\n",
        "        )\n",
        "    )\n",
        "    if apply_norm:\n",
        "        block.add(tfa.layers.InstanceNormalization())\n",
        "    if apply_activation:\n",
        "        block.add(layers.LeakyReLU(0.2))\n",
        "    if apply_dropout:\n",
        "        block.add(layers.Dropout(0.5))\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_encoder(image_shape, encoder_downsample_factor=64, latent_dim=256):\n",
        "    input_image = keras.Input(shape=image_shape)\n",
        "    x = downsample(encoder_downsample_factor, 3, apply_norm=False)(input_image)\n",
        "    x = downsample(2 * encoder_downsample_factor, 3)(x)\n",
        "    x = downsample(4 * encoder_downsample_factor, 3)(x)\n",
        "    x = downsample(8 * encoder_downsample_factor, 3)(x)\n",
        "    x = downsample(8 * encoder_downsample_factor, 3)(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    mean = layers.Dense(latent_dim, name=\"mean\")(x)\n",
        "    variance = layers.Dense(latent_dim, name=\"variance\")(x)\n",
        "    plot_model(keras.Model(input_image, [mean, variance], name=\"encoder\"), \"encoder.png\",\n",
        "               show_shapes=True, show_layer_names=True, show_layer_activations=True,\n",
        "               expand_nested=True)\n",
        "    return keras.Model(input_image, [mean, variance], name=\"encoder\")\n",
        "\n",
        "\n",
        "def build_generator(mask_shape, latent_dim=256):\n",
        "    latent = keras.Input(shape=(latent_dim))\n",
        "    mask = keras.Input(shape=mask_shape)\n",
        "    x = layers.Dense(16384)(latent)\n",
        "    x = layers.Reshape((4, 4, 1024))(x)\n",
        "    x = ResBlock(filters=1024)(x, mask)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "    x = ResBlock(filters=1024)(x, mask)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "    x = ResBlock(filters=1024)(x, mask)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "    x = ResBlock(filters=512)(x, mask)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "    x = ResBlock(filters=256)(x, mask)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "    x = ResBlock(filters=128)(x, mask)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "    x = ResBlock(filters=64)(x, mask)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "    x = ResBlock(filters=32)(x, mask)\n",
        "    x = tf.nn.leaky_relu(x, 0.2)\n",
        "    output_image = tf.nn.tanh(layers.Conv2D(3, 4, padding=\"same\")(x))\n",
        "    plot_model(keras.Model([latent, mask], output_image, name=\"generator\"), \"generator.png\",\n",
        "               show_shapes=True, show_layer_names=True, show_layer_activations=True,\n",
        "               expand_nested=True)\n",
        "    return keras.Model([latent, mask], output_image, name=\"generator\")\n",
        "\n",
        "\n",
        "def build_discriminator(image_shape, downsample_factor=64):\n",
        "    input_image_A = keras.Input(shape=image_shape, name=\"discriminator_image_A\")\n",
        "    input_image_B = keras.Input(shape=image_shape, name=\"discriminator_image_B\")\n",
        "    x = layers.Concatenate()([input_image_A, input_image_B])\n",
        "    x1 = downsample(downsample_factor, 4, apply_norm=False)(x)\n",
        "    x2 = downsample(2 * downsample_factor, 4)(x1)\n",
        "    x3 = downsample(4 * downsample_factor, 4)(x2)\n",
        "    x4 = downsample(8 * downsample_factor, 4, strides=1)(x3)\n",
        "    x5 = layers.Conv2D(1, 4)(x4)\n",
        "    outputs = [x1, x2, x3, x4, x5]\n",
        "    plot_model(keras.Model([input_image_A, input_image_B], outputs), \"discriminator.png\",\n",
        "               show_shapes=True, show_layer_names=True, show_layer_activations=True,\n",
        "               expand_nested=True)\n",
        "    return keras.Model([input_image_A, input_image_B], outputs, name=\"discriminator\")\n",
        "\n",
        "\n",
        "def generator_loss(y):\n",
        "    return -tf.reduce_mean(y)\n",
        "\n",
        "\n",
        "def kl_divergence_loss(mean, variance):\n",
        "    return -0.5 * tf.reduce_sum(1 + variance - tf.square(mean) - tf.exp(variance))\n",
        "\n",
        "\n",
        "class FeatureMatchingLoss(keras.losses.Loss):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.mae = keras.losses.MeanAbsoluteError()\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        loss = 0\n",
        "        for i in range(len(y_true) - 1):\n",
        "            loss += self.mae(y_true[i], y_pred[i])\n",
        "        return loss\n",
        "\n",
        "\n",
        "class VGGFeatureMatchingLoss(keras.losses.Loss):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder_layers = [\n",
        "            \"block1_conv1\",\n",
        "            \"block2_conv1\",\n",
        "            \"block3_conv1\",\n",
        "            \"block4_conv1\",\n",
        "            \"block5_conv1\",\n",
        "        ]\n",
        "        self.weights = [1.0 / 32, 1.0 / 16, 1.0 / 8, 1.0 / 4, 1.0]\n",
        "        vgg = keras.applications.VGG19(include_top=False, weights=\"imagenet\")\n",
        "        layer_outputs = [vgg.get_layer(x).output for x in self.encoder_layers]\n",
        "        self.vgg_model = keras.Model(vgg.input, layer_outputs, name=\"VGG\")\n",
        "        self.mae = keras.losses.MeanAbsoluteError()\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_true = keras.applications.vgg19.preprocess_input(127.5 * (y_true + 1))\n",
        "        y_pred = keras.applications.vgg19.preprocess_input(127.5 * (y_pred + 1))\n",
        "        real_features = self.vgg_model(y_true)\n",
        "        fake_features = self.vgg_model(y_pred)\n",
        "        loss = 0\n",
        "        for i in range(len(real_features)):\n",
        "            loss += self.weights[i] * self.mae(real_features[i], fake_features[i])\n",
        "        return loss\n",
        "\n",
        "\n",
        "class DiscriminatorLoss(keras.losses.Loss):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hinge_loss = keras.losses.Hinge()\n",
        "\n",
        "    def call(self, y, is_real):\n",
        "        label = 1.0 if is_real else -1.0\n",
        "        return self.hinge_loss(label, y)\n",
        "\n",
        "\n",
        "class GanMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, val_dataset, n_samples, epoch_interval=300):\n",
        "        self.val_images = next(iter(val_dataset))\n",
        "        self.n_samples = n_samples\n",
        "        self.epoch_interval = epoch_interval\n",
        "\n",
        "    def infer(self):\n",
        "        latent_vector = tf.random.normal(\n",
        "            shape=(self.model.batch_size, self.model.latent_dim), mean=0.0, stddev=2.0\n",
        "        )\n",
        "        return self.model.predict([latent_vector, self.val_images[2]])\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch % self.epoch_interval == 0:\n",
        "            generated_images = self.infer()\n",
        "            for _ in range(self.n_samples):\n",
        "                grid_row = min(generated_images.shape[0], 3)\n",
        "                f, axarr = plt.subplots(grid_row, 3, figsize=(18, grid_row * 6))\n",
        "                for row in range(grid_row):\n",
        "                    ax = axarr if grid_row == 1 else axarr[row]\n",
        "                    ax[0].imshow((self.val_images[0][row] + 1) / 2)\n",
        "                    ax[0].axis(\"off\")\n",
        "                    ax[0].set_title(\"Mask\", fontsize=20)\n",
        "                    ax[1].imshow((self.val_images[1][row] + 1) / 2)\n",
        "                    ax[1].axis(\"off\")\n",
        "                    ax[1].set_title(\"Ground Truth\", fontsize=20)\n",
        "                    ax[2].imshow((generated_images[row] + 1) / 2)\n",
        "                    ax[2].axis(\"off\")\n",
        "                    ax[2].set_title(\"Generated\", fontsize=20)\n",
        "                plt.savefig(\".epochImages/\"f\"{epoch}\")\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "class GauGAN(keras.Model):\n",
        "    def __init__(\n",
        "            self,\n",
        "            image_size,\n",
        "            num_classes,\n",
        "            batch_size,\n",
        "            latent_dim,\n",
        "            feature_loss_coeff=10,\n",
        "            vgg_feature_loss_coeff=0.1,\n",
        "            kl_divergence_loss_coeff=0.1,\n",
        "            **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.num_classes = num_classes\n",
        "        self.image_shape = (image_size, image_size, 3)\n",
        "        self.mask_shape = (image_size, image_size, num_classes)\n",
        "        self.feature_loss_coeff = feature_loss_coeff\n",
        "        self.vgg_feature_loss_coeff = vgg_feature_loss_coeff\n",
        "        self.kl_divergence_loss_coeff = kl_divergence_loss_coeff\n",
        "\n",
        "        self.discriminator = build_discriminator(self.image_shape)\n",
        "        self.generator = build_generator(self.mask_shape, latent_dim=self.latent_dim)\n",
        "        self.encoder = build_encoder(self.image_shape, latent_dim=self.latent_dim)\n",
        "        self.sampler = GaussianSampler(batch_size, latent_dim)\n",
        "        self.patch_size, self.combined_model = self.build_combined_generator()\n",
        "\n",
        "        self.disc_loss_tracker = tf.keras.metrics.Mean(name=\"disc_loss\")\n",
        "        self.gen_loss_tracker = tf.keras.metrics.Mean(name=\"gen_loss\")\n",
        "        self.feat_loss_tracker = tf.keras.metrics.Mean(name=\"feat_loss\")\n",
        "        self.vgg_loss_tracker = tf.keras.metrics.Mean(name=\"vgg_loss\")\n",
        "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.disc_loss_tracker,\n",
        "            self.gen_loss_tracker,\n",
        "            self.feat_loss_tracker,\n",
        "            self.vgg_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def build_combined_generator(self):\n",
        "        # This method builds a model that takes as inputs the following:\n",
        "        # latent vector, one-hot encoded segmentation label map, and\n",
        "        # a segmentation map. It then (i) generates an image with the generator,\n",
        "        # (ii) passes the generated images and segmentation map to the discriminator.\n",
        "        # Finally, the model produces the following outputs: (a) discriminator outputs,\n",
        "        # (b) generated image.\n",
        "        # We will be using this model to simplify the implementation.\n",
        "        self.discriminator.trainable = False\n",
        "        mask_input = keras.Input(shape=self.mask_shape, name=\"mask\")\n",
        "        image_input = keras.Input(shape=self.image_shape, name=\"image\")\n",
        "        latent_input = keras.Input(shape=(self.latent_dim), name=\"latent\")\n",
        "        generated_image = self.generator([latent_input, mask_input])\n",
        "        discriminator_output = self.discriminator([image_input, generated_image])\n",
        "        patch_size = discriminator_output[-1].shape[1]\n",
        "        combined_model = keras.Model(\n",
        "            [latent_input, mask_input, image_input],\n",
        "            [discriminator_output, generated_image],\n",
        "        )\n",
        "        plot_model(combined_model, \"combined_model.png\",\n",
        "                   show_shapes=True, show_layer_names=True, show_layer_activations=True,\n",
        "                   expand_nested=False)\n",
        "        return patch_size, combined_model\n",
        "\n",
        "    def compile(self, gen_lr=1e-4, disc_lr=4e-4, **kwargs):\n",
        "        super().compile(**kwargs)\n",
        "        self.generator_optimizer = keras.optimizers.Adam(\n",
        "            gen_lr, beta_1=0.0, beta_2=0.999\n",
        "        )\n",
        "        self.discriminator_optimizer = keras.optimizers.Adam(\n",
        "            disc_lr, beta_1=0.0, beta_2=0.999\n",
        "        )\n",
        "        self.discriminator_loss = DiscriminatorLoss()\n",
        "        self.feature_matching_loss = FeatureMatchingLoss()\n",
        "        self.vgg_loss = VGGFeatureMatchingLoss()\n",
        "\n",
        "    def train_discriminator(self, latent_vector, segmentation_map, real_image, labels):\n",
        "        fake_images = self.generator([latent_vector, labels])\n",
        "        with tf.GradientTape() as gradient_tape:\n",
        "            pred_fake = self.discriminator([segmentation_map, fake_images])[-1]\n",
        "            pred_real = self.discriminator([segmentation_map, real_image])[-1]\n",
        "            loss_fake = self.discriminator_loss(pred_fake, False)\n",
        "            loss_real = self.discriminator_loss(pred_real, True)\n",
        "            total_loss = 0.5 * (loss_fake + loss_real)\n",
        "\n",
        "        self.discriminator.trainable = True\n",
        "        gradients = gradient_tape.gradient(\n",
        "            total_loss, self.discriminator.trainable_variables\n",
        "        )\n",
        "        self.discriminator_optimizer.apply_gradients(\n",
        "            zip(gradients, self.discriminator.trainable_variables)\n",
        "        )\n",
        "        return total_loss\n",
        "\n",
        "    def train_generator(\n",
        "            self, latent_vector, segmentation_map, labels, image, mean, variance\n",
        "    ):\n",
        "        # Generator learns through the signal provided by the discriminator. During\n",
        "        # backpropagation, we only update the generator parameters.\n",
        "        self.discriminator.trainable = False\n",
        "        with tf.GradientTape() as tape:\n",
        "            real_d_output = self.discriminator([segmentation_map, image])\n",
        "            fake_d_output, fake_image = self.combined_model(\n",
        "                [latent_vector, labels, segmentation_map]\n",
        "            )\n",
        "            pred = fake_d_output[-1]\n",
        "\n",
        "            # Compute generator losses.\n",
        "            g_loss = generator_loss(pred)\n",
        "            kl_loss = self.kl_divergence_loss_coeff * kl_divergence_loss(mean, variance)\n",
        "            vgg_loss = self.vgg_feature_loss_coeff * self.vgg_loss(image, fake_image)\n",
        "            feature_loss = self.feature_loss_coeff * self.feature_matching_loss(\n",
        "                real_d_output, fake_d_output\n",
        "            )\n",
        "            total_loss = g_loss + kl_loss + vgg_loss + feature_loss\n",
        "\n",
        "        all_trainable_variables = (\n",
        "                self.combined_model.trainable_variables + self.encoder.trainable_variables\n",
        "        )\n",
        "\n",
        "        gradients = tape.gradient(total_loss, all_trainable_variables)\n",
        "        self.generator_optimizer.apply_gradients(\n",
        "            zip(gradients, all_trainable_variables)\n",
        "        )\n",
        "        return total_loss, feature_loss, vgg_loss, kl_loss\n",
        "\n",
        "    def train_step(self, data):\n",
        "        segmentation_map, image, labels = data\n",
        "        mean, variance = self.encoder(image)\n",
        "        latent_vector = self.sampler([mean, variance])\n",
        "        discriminator_loss = self.train_discriminator(\n",
        "            latent_vector, segmentation_map, image, labels\n",
        "        )\n",
        "        (generator_loss, feature_loss, vgg_loss, kl_loss) = self.train_generator(\n",
        "            latent_vector, segmentation_map, labels, image, mean, variance\n",
        "        )\n",
        "\n",
        "        # Report progress.\n",
        "        self.disc_loss_tracker.update_state(discriminator_loss)\n",
        "        self.gen_loss_tracker.update_state(generator_loss)\n",
        "        self.feat_loss_tracker.update_state(feature_loss)\n",
        "        self.vgg_loss_tracker.update_state(vgg_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        segmentation_map, image, labels = data\n",
        "        # Obtain the learned moments of the real image distribution.\n",
        "        mean, variance = self.encoder(image)\n",
        "\n",
        "        # Sample a latent from the distribution defined by the learned moments.\n",
        "        latent_vector = self.sampler([mean, variance])\n",
        "\n",
        "        # Generate the fake images.\n",
        "        fake_images = self.generator([latent_vector, labels])\n",
        "\n",
        "        # Calculate the losses.\n",
        "        pred_fake = self.discriminator([segmentation_map, fake_images])[-1]\n",
        "        pred_real = self.discriminator([segmentation_map, image])[-1]\n",
        "        loss_fake = self.discriminator_loss(pred_fake, False)\n",
        "        loss_real = self.discriminator_loss(pred_real, True)\n",
        "        total_discriminator_loss = 0.5 * (loss_fake + loss_real)\n",
        "        real_d_output = self.discriminator([segmentation_map, image])\n",
        "        fake_d_output, fake_image = self.combined_model(\n",
        "            [latent_vector, labels, segmentation_map]\n",
        "        )\n",
        "        pred = fake_d_output[-1]\n",
        "        g_loss = generator_loss(pred)\n",
        "        kl_loss = self.kl_divergence_loss_coeff * kl_divergence_loss(mean, variance)\n",
        "        vgg_loss = self.vgg_feature_loss_coeff * self.vgg_loss(image, fake_image)\n",
        "        feature_loss = self.feature_loss_coeff * self.feature_matching_loss(real_d_output, fake_d_output)\n",
        "        total_generator_loss = g_loss + kl_loss + vgg_loss + feature_loss\n",
        "\n",
        "        # Report progress.\n",
        "        self.disc_loss_tracker.update_state(total_discriminator_loss)\n",
        "        self.gen_loss_tracker.update_state(total_generator_loss)\n",
        "        self.feat_loss_tracker.update_state(feature_loss)\n",
        "        self.vgg_loss_tracker.update_state(vgg_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        return results\n",
        "\n",
        "    def call(self, inputs):\n",
        "        latent_vectors, labels = inputs\n",
        "        return self.generator([latent_vectors, labels])\n",
        "\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 100:\n",
        "        return lr\n",
        "    elif epoch == 100:\n",
        "        return lr * 0.5\n",
        "    elif epoch == 150:\n",
        "        return lr * 0.5\n",
        "    elif epoch == 200:\n",
        "        return lr * 0.5\n",
        "    elif epoch == 250:\n",
        "        return lr * 0.5\n",
        "    else:\n",
        "        return lr\n",
        "\n",
        "\n",
        "my_callbacks = [\n",
        "    #GanMonitor(val_dataset, BATCH_SIZE),\n",
        "    ModelCheckpoint(filepath='./SavedModels/model.{epoch:02d}', save_freq=4750),\n",
        "    #LearningRateScheduler(scheduler),\n",
        "]\n",
        "\n",
        "gaugan = GauGAN(IMG_HEIGHT, NUM_CLASSES, BATCH_SIZE, latent_dim=512)\n",
        "gaugan.compile(run_eagerly=False, weighted_metrics=[])\n",
        "history = gaugan.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=200,\n",
        "    callbacks=my_callbacks,\n",
        ")\n",
        "gaugan.save(\"savedModel\")\n",
        "\n",
        "val_iterator = iter(val_dataset)\n",
        "\n",
        "for _ in range(5):\n",
        "    val_images = next(val_iterator)\n",
        "    # Sample latent from a normal distribution.\n",
        "    latent_vector = tf.random.normal(\n",
        "        shape=(gaugan.batch_size, gaugan.latent_dim), mean=0.0, stddev=2.0\n",
        "    )\n",
        "    # Generate fake images.\n",
        "    fake_images = gaugan.predict([latent_vector, val_images[2]])\n",
        "\n",
        "    real_images = val_images\n",
        "    grid_row = min(fake_images.shape[0], 3)\n",
        "    grid_col = 3\n",
        "    f, axarr = plt.subplots(grid_row, grid_col, figsize=(grid_col * 6, grid_row * 6))\n",
        "    for row in range(grid_row):\n",
        "        ax = axarr if grid_row == 1 else axarr[row]\n",
        "        ax[0].imshow((real_images[0][row] + 1) / 2)\n",
        "        ax[0].axis(\"off\")\n",
        "        ax[0].set_title(\"Mask\", fontsize=20)\n",
        "        ax[1].imshow((real_images[1][row] + 1) / 2)\n",
        "        ax[1].axis(\"off\")\n",
        "        ax[1].set_title(\"Ground Truth\", fontsize=20)\n",
        "        ax[2].imshow((fake_images[row] + 1) / 2)\n",
        "        ax[2].axis(\"off\")\n",
        "        ax[2].set_title(\"Generated\", fontsize=20)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_history(item):\n",
        "    plt.plot(history.history[item], label=item)\n",
        "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(item)\n",
        "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_history(\"disc_loss\")\n",
        "plot_history(\"gen_loss\")\n",
        "plot_history(\"feat_loss\")\n",
        "plot_history(\"vgg_loss\")\n",
        "plot_history(\"kl_loss\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMK+oO6vTWXy1JqM2oJi3z3",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}